{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eda1e1-e171-427c-bb19-6cb8137cecef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T00:12:41.015177Z",
     "iopub.status.busy": "2026-02-09T00:12:41.014687Z",
     "iopub.status.idle": "2026-02-09T00:12:41.023833Z",
     "shell.execute_reply": "2026-02-09T00:12:41.022989Z",
     "shell.execute_reply.started": "2026-02-09T00:12:41.015146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from brainsmash.workbench.geo import parcellate\n",
    "from brainsmash.mapgen.base import Base\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "import anndata\n",
    "from typing import List, Dict\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import os\n",
    "import scdrs\n",
    "import scdrs.method as md\n",
    "import scipy as sp\n",
    "import zoom.prepare as pp\n",
    "from zoom import ZOOM\n",
    "import zoom.sc_tool as sct\n",
    "\n",
    "os.chdir(\"/slurm/home/yrd/liaolab/nieshuyang/AHBA_sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b97b6-c039-4933-b9da-94a1d0ecc511",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-02-08T16:39:49.914172Z",
     "iopub.status.busy": "2026-02-08T16:39:49.913564Z",
     "iopub.status.idle": "2026-02-08T17:30:25.443703Z",
     "shell.execute_reply": "2026-02-08T17:30:25.443012Z",
     "shell.execute_reply.started": "2026-02-08T16:39:49.914136Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing parcellated distance matrix\n",
      "\n",
      "For a 32k-vertex cortical hemisphere, this takes about 30 mins for the HCP MMP parcellation. For subcortex, this takes about an hour for the CAB-NP parcellation.\n",
      "WARNING: overwriting /slurm/home/yrd/liaolab/nieshuyang/AHBA_sc/HCPMMP/LeftParcelGeodesicDistmat.txt\n",
      "# Parcel label 181.0 complete.\n",
      "# Parcel label 182.0 complete.\n",
      "# Parcel label 183.0 complete.\n",
      "# Parcel label 184.0 complete.\n",
      "# Parcel label 185.0 complete.\n",
      "# Parcel label 186.0 complete.\n",
      "# Parcel label 187.0 complete.\n",
      "# Parcel label 188.0 complete.\n",
      "# Parcel label 189.0 complete.\n",
      "# Parcel label 190.0 complete.\n",
      "# Parcel label 191.0 complete.\n",
      "# Parcel label 192.0 complete.\n",
      "# Parcel label 193.0 complete.\n",
      "# Parcel label 199.0 complete.\n",
      "# Parcel label 200.0 complete.\n",
      "# Parcel label 201.0 complete.\n",
      "# Parcel label 202.0 complete.\n",
      "# Parcel label 203.0 complete.\n",
      "# Parcel label 204.0 complete.\n",
      "# Parcel label 205.0 complete.\n",
      "# Parcel label 206.0 complete.\n",
      "# Parcel label 207.0 complete.\n",
      "# Parcel label 208.0 complete.\n",
      "# Parcel label 209.0 complete.\n",
      "# Parcel label 210.0 complete.\n",
      "# Parcel label 211.0 complete.\n",
      "# Parcel label 212.0 complete.\n",
      "# Parcel label 213.0 complete.\n",
      "# Parcel label 214.0 complete.\n",
      "# Parcel label 215.0 complete.\n",
      "# Parcel label 216.0 complete.\n",
      "# Parcel label 217.0 complete.\n",
      "# Parcel label 218.0 complete.\n",
      "# Parcel label 219.0 complete.\n",
      "# Parcel label 220.0 complete.\n",
      "# Parcel label 221.0 complete.\n",
      "# Parcel label 222.0 complete.\n",
      "# Parcel label 223.0 complete.\n",
      "# Parcel label 224.0 complete.\n",
      "# Parcel label 225.0 complete.\n",
      "# Parcel label 226.0 complete.\n",
      "# Parcel label 227.0 complete.\n",
      "# Parcel label 228.0 complete.\n",
      "# Parcel label 229.0 complete.\n",
      "# Parcel label 230.0 complete.\n",
      "# Parcel label 231.0 complete.\n",
      "# Parcel label 232.0 complete.\n",
      "# Parcel label 233.0 complete.\n",
      "# Parcel label 234.0 complete.\n",
      "# Parcel label 235.0 complete.\n",
      "# Parcel label 236.0 complete.\n",
      "# Parcel label 237.0 complete.\n",
      "# Parcel label 238.0 complete.\n",
      "# Parcel label 239.0 complete.\n",
      "# Parcel label 240.0 complete.\n",
      "# Parcel label 241.0 complete.\n",
      "# Parcel label 242.0 complete.\n",
      "# Parcel label 243.0 complete.\n",
      "# Parcel label 244.0 complete.\n",
      "# Parcel label 245.0 complete.\n",
      "# Parcel label 246.0 complete.\n",
      "# Parcel label 247.0 complete.\n",
      "# Parcel label 248.0 complete.\n",
      "# Parcel label 249.0 complete.\n",
      "# Parcel label 250.0 complete.\n",
      "# Parcel label 251.0 complete.\n",
      "# Parcel label 252.0 complete.\n",
      "# Parcel label 253.0 complete.\n",
      "# Parcel label 254.0 complete.\n",
      "# Parcel label 255.0 complete.\n",
      "# Parcel label 256.0 complete.\n",
      "# Parcel label 257.0 complete.\n",
      "# Parcel label 258.0 complete.\n",
      "# Parcel label 259.0 complete.\n",
      "# Parcel label 260.0 complete.\n",
      "# Parcel label 261.0 complete.\n",
      "# Parcel label 262.0 complete.\n",
      "# Parcel label 263.0 complete.\n",
      "# Parcel label 264.0 complete.\n",
      "# Parcel label 265.0 complete.\n",
      "# Parcel label 266.0 complete.\n",
      "# Parcel label 267.0 complete.\n",
      "# Parcel label 268.0 complete.\n",
      "# Parcel label 269.0 complete.\n",
      "# Parcel label 270.0 complete.\n",
      "# Parcel label 271.0 complete.\n",
      "# Parcel label 272.0 complete.\n",
      "# Parcel label 273.0 complete.\n",
      "# Parcel label 274.0 complete.\n",
      "# Parcel label 275.0 complete.\n",
      "# Parcel label 276.0 complete.\n",
      "# Parcel label 277.0 complete.\n",
      "# Parcel label 278.0 complete.\n",
      "# Parcel label 279.0 complete.\n",
      "# Parcel label 280.0 complete.\n",
      "# Parcel label 281.0 complete.\n",
      "# Parcel label 282.0 complete.\n",
      "# Parcel label 283.0 complete.\n",
      "# Parcel label 284.0 complete.\n",
      "# Parcel label 285.0 complete.\n",
      "# Parcel label 286.0 complete.\n",
      "# Parcel label 287.0 complete.\n",
      "# Parcel label 288.0 complete.\n",
      "# Parcel label 289.0 complete.\n",
      "# Parcel label 290.0 complete.\n",
      "# Parcel label 291.0 complete.\n",
      "# Parcel label 292.0 complete.\n",
      "# Parcel label 293.0 complete.\n",
      "# Parcel label 294.0 complete.\n",
      "# Parcel label 295.0 complete.\n",
      "# Parcel label 296.0 complete.\n",
      "# Parcel label 297.0 complete.\n",
      "# Parcel label 298.0 complete.\n",
      "# Parcel label 299.0 complete.\n",
      "# Parcel label 300.0 complete.\n",
      "# Parcel label 301.0 complete.\n",
      "# Parcel label 302.0 complete.\n",
      "# Parcel label 303.0 complete.\n",
      "# Parcel label 304.0 complete.\n",
      "# Parcel label 305.0 complete.\n",
      "# Parcel label 306.0 complete.\n",
      "# Parcel label 307.0 complete.\n",
      "# Parcel label 308.0 complete.\n",
      "# Parcel label 309.0 complete.\n",
      "# Parcel label 310.0 complete.\n",
      "# Parcel label 311.0 complete.\n",
      "# Parcel label 312.0 complete.\n",
      "# Parcel label 313.0 complete.\n",
      "# Parcel label 314.0 complete.\n",
      "# Parcel label 315.0 complete.\n",
      "# Parcel label 316.0 complete.\n",
      "# Parcel label 317.0 complete.\n",
      "# Parcel label 318.0 complete.\n",
      "# Parcel label 319.0 complete.\n",
      "# Parcel label 320.0 complete.\n",
      "# Parcel label 321.0 complete.\n",
      "# Parcel label 322.0 complete.\n",
      "# Parcel label 323.0 complete.\n",
      "# Parcel label 324.0 complete.\n",
      "# Parcel label 325.0 complete.\n",
      "# Parcel label 326.0 complete.\n",
      "# Parcel label 327.0 complete.\n",
      "# Parcel label 328.0 complete.\n",
      "# Parcel label 329.0 complete.\n",
      "# Parcel label 330.0 complete.\n",
      "# Parcel label 331.0 complete.\n",
      "# Parcel label 332.0 complete.\n",
      "# Parcel label 333.0 complete.\n",
      "# Parcel label 334.0 complete.\n",
      "# Parcel label 335.0 complete.\n",
      "# Parcel label 336.0 complete.\n",
      "# Parcel label 337.0 complete.\n",
      "# Parcel label 338.0 complete.\n",
      "# Parcel label 339.0 complete.\n",
      "# Parcel label 340.0 complete.\n",
      "# Parcel label 341.0 complete.\n",
      "# Parcel label 342.0 complete.\n",
      "# Parcel label 343.0 complete.\n",
      "# Parcel label 344.0 complete.\n",
      "# Parcel label 345.0 complete.\n",
      "# Parcel label 346.0 complete.\n",
      "# Parcel label 347.0 complete.\n",
      "# Parcel label 348.0 complete.\n",
      "# Parcel label 349.0 complete.\n",
      "# Parcel label 350.0 complete.\n",
      "# Parcel label 351.0 complete.\n",
      "# Parcel label 352.0 complete.\n",
      "# Parcel label 353.0 complete.\n",
      "# Parcel label 354.0 complete.\n",
      "# Parcel label 355.0 complete.\n",
      "# Parcel label 356.0 complete.\n",
      "# Parcel label 357.0 complete.\n",
      "# Parcel label 358.0 complete.\n",
      "# Parcel label 359.0 complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/slurm/home/yrd/liaolab/nieshuyang/AHBA_sc/HCPMMP/LeftParcelGeodesicDistmat.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare BrainSMASH\n",
    "infile = \"/HCPMMP/LeftDenseGeodesicDistmat.txt\"\n",
    "outfile = \"/HCPMMP/LeftParcelGeodesicDistmat.txt\"\n",
    "dlabel = \"/HCPMMP/lh.HCPMMP1.fsLR.32k.dlabel.nii\"\n",
    "parcellate(infile, dlabel, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be3683-bbdd-4170-beac-0ad2fe0bb3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of surrogate maps for each SBP\n",
    "SBPs = [\"UTG\",\"SVG\",\"MEGtheta\",\"MOR\",\"KOR\",\"CMRO2\",\"T1T2\",\"deltaMT\"]\n",
    "np.random.seed(123)\n",
    "arr = np.random.rand(len(SBPs))\n",
    "arr = arr / arr.sum() * 100\n",
    "arr = np.round(arr).astype(int)\n",
    "diff = 100 - arr.sum()\n",
    "arr[0] += diff\n",
    "surr_num = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d13b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate null SBPs based on valid SBPs through BrianSMASH\n",
    "D = np.loadtxt(outfile)\n",
    "template = nib.load(\"/Benchmark/Null_simulation/lh.template.fsLR.32k.func.gii\")\n",
    "null_counts = 1\n",
    "\n",
    "def map_HCPMMP(surr):\n",
    "    HCPMMP = nib.load('/HCPMMP/lh.HCPMMP1.fsLR.32k.label.gii')\n",
    "    HCPMMP = HCPMMP.darrays[0].data.astype(int)\n",
    "    mapping = np.zeros(HCPMMP.max() + 1) \n",
    "    mapping[1:] = surr\n",
    "    return mapping[HCPMMP]\n",
    "\n",
    "for i in range(len(SBPs)):\n",
    "    SBP = SBPs[i]\n",
    "    brain_map = pd.read_csv(f\"/HCPMMP/{SBP}/{SBP}_HCPMMP.csv\", index_col=0)\n",
    "    brain_map = brain_map[brain_map.index!=0]\n",
    "    brain_map = brain_map[\"SBP\"].values\n",
    "    # Use parcellation version\n",
    "    base = Base(x=brain_map, D=D, resample=True)\n",
    "    surrogates = base(n=surr_num[i])\n",
    "    # Save GIFTI format null SBPs\n",
    "    for j in range(surrogates.shape[0]):\n",
    "        surr = surrogates[i,:]\n",
    "        surr = map_HCPMMP(surr)\n",
    "        template.darrays[0].data = surr\n",
    "        nib.save(template, f\"/Benchmark/Null_simulation/Null_SBP/null{null_counts}.func.gii\")\n",
    "        null_counts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0753fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare spatial autocorrelation preserving gene sets for benchmarking\n",
    "for n in range(1,101):\n",
    "    null_parc, null_perm_parc = pp.process_SBP(\n",
    "        SBP=f\"=/Benchmark/Null_simulation/Null_SBP/null{n}.func.gii\",\n",
    "        parcellation=\"/HCPMMP/lh.HCPMMP1.fsLR.32k.label.gii\",\n",
    "        atlas=\"fsLR\", density=\"32k\", hemi=\"L\", n_perm=1000, seed=123\n",
    "    )\n",
    "    #null_parc.to_csv(f\"/Benchmark/Null_simulation/Null_SBP/null{n}_HCPMMP.csv\")\n",
    "    #null_perm_parc.to_csv(f\"/Benchmark/Null_simulation/Null_SBP/null{n}_perm_HCPMMP.csv\")\n",
    "    zoom_null = ZOOM(\n",
    "        expression=\"/HCPMMP/expression_HCPMMP.csv\",\n",
    "        SBP=null_parc, SBP_perm=null_perm_parc,\n",
    "        best_comp=1 # Set component number = 1 for simplicity\n",
    "    )\n",
    "    zoom_null.get_gene_contrib()\n",
    "    zoom_null.PLS_report.to_csv(f\"/Benchmark/Null_simulation/Null_SBP/null{n}_PLS_report.csv\") # Gene set and corresponding weights\n",
    "    zoom_null.weight_perm.to_csv(f\"/Benchmark/Null_simulation/Null_SBP/null{n}_weight_perm.csv\") # Used for ZOOM scoring\n",
    "    zoom_null.sign_perm.to_csv(f\"/Benchmark/Null_simulation/Null_SBP/null{n}_sign_perm.csv\") # Used for ZOOM scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for benchmarking\n",
    "adata = sc.read_h5ad(\"/scRNA/adult_ctx_gss.h5ad\")\n",
    "# Smaple 100,000 cells\n",
    "np.random.seed(123)\n",
    "sampled_idx = np.random.choice(adata.n_obs, 100000, replace=False)\n",
    "adata = adata[sampled_idx,:]\n",
    "adata.obs.to_csv(\"/scRNA/benchmarking_100000cells.csv\")\n",
    "adata.write(\"/scRNA/benchmarking_100000cells.h5ad\")\n",
    "# Covariate matrix\n",
    "selected_columns = ['Area.v2', 'Dataset', 'nFeature_RNA']\n",
    "sc_cov = adata.obs[selected_columns].copy()\n",
    "sc_cov['Area.v2'], _ = pd.factorize(sc_cov['Area.v2'])\n",
    "sc_cov['Dataset'], _ = pd.factorize(sc_cov['Dataset'])\n",
    "sc_cov['const'] = 1\n",
    "sc_cov.index.name = 'index'\n",
    "sc_cov = sc_cov.astype(np.float32)\n",
    "\n",
    "gene_nums = [30,50,75,100,150,200,250,300,400,500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe904c",
   "metadata": {},
   "source": [
    "Seurat & VISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7bbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess to regress out covariates\n",
    "cov_list = list(sc_cov.columns)\n",
    "adata.obs.drop([x for x in cov_list if x in adata.obs.columns], axis=1, inplace=True)\n",
    "adata.obs = adata.obs.join(sc_cov)\n",
    "adata.obs.fillna(adata.obs[cov_list].mean(), inplace=True)\n",
    "adata.X = adata.layers[\"data\"]\n",
    "adata.var['mean'] = adata.X.mean(axis=0).T\n",
    "if sp.sparse.issparse(adata.X):\n",
    "    adata.X = adata.X.toarray()\n",
    "adata.X -= adata.var['mean'].values\n",
    "adata.X = scdrs.pp.reg_out(adata.X, adata.obs[cov_list].values)\n",
    "adata.X += adata.var['mean'].values\n",
    "\n",
    "# Scoring and get p-value distributions\n",
    "for gene_num in gene_nums:\n",
    "    VISION_pvals = pd.DataFrame(index=adata.obs.index)\n",
    "    Seurat_pvals = pd.DataFrame(index=adata.obs.index)\n",
    "    for n in range(1,101):\n",
    "        gene_rep_null = pd.read_csv(f\"/Benchmark/Null_simulation/Null_SBP_new/null{n}_PLS_report.csv\",index_col=0)\n",
    "        gene_rep_null = gene_rep_null[gene_rep_null[\"Sign\"]>0]\n",
    "        gene_list = list(gene_rep_null.head(gene_num).index)\n",
    "        VISION_null_n = md.score_cell_vision(adata, gene_list)\n",
    "        VISION_pvals[f\"pval_{n}\"] = VISION_null_n[\"norm_pval\"]\n",
    "        Seurat_null_n = md.score_cell_scanpy(adata, gene_list) # The scoring function of scanpy is the same as Seurat\n",
    "        Seurat_pvals[f\"pval_{n}\"] = Seurat_null_n[\"pval\"]\n",
    "    VISION_pvals.to_csv(f\"/Benchmark/Null_simulation/VISION_pvals_{gene_num}genes.csv\")\n",
    "    Seurat_pvals.to_csv(f\"/Benchmark/Null_simulation/Seurat_pvals_{gene_num}genes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f5498",
   "metadata": {},
   "source": [
    "scDRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ebcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a copy of scdrs.method. We further employ joblib to accelerate the core scoring function of scDRS\n",
    "def score_cell_parallel(\n",
    "    data,\n",
    "    gene_list,\n",
    "    gene_weight=None,\n",
    "    ctrl_match_key=\"mean_var\",\n",
    "    n_ctrl=1000,\n",
    "    n_genebin=200,\n",
    "    weight_opt=\"vs\",\n",
    "    copy=False,\n",
    "    return_ctrl_raw_score=False,\n",
    "    return_ctrl_norm_score=False,\n",
    "    random_seed=0,\n",
    "    verbose=False,\n",
    "    save_intermediate=None,\n",
    "):\n",
    "\n",
    "    \"\"\"Score cells based on the disease gene set.\n",
    "\n",
    "    Preprocessing information `data.uns[\"SCDRS_PARAM\"]` is required\n",
    "    (run `scdrs.pp.preprocess` first).\n",
    "\n",
    "    It operates in implicit-covariate-correction mode if both `FLAG_SPARSE`\n",
    "    and `FLAG_COV` are `True`, where computations are based on the implicit\n",
    "    covariate-corrected data\n",
    "\n",
    "        `CORRECTED_X = data.X + COV_MAT * COV_BETA + COV_GENE_MEAN`.\n",
    "\n",
    "    It operates in normal mode otherwise, where computations are based on `data.X`,\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : anndata.AnnData\n",
    "        Single-cell data of shape (n_cell, n_gene). Assumed\n",
    "        to be size-factor-normalized and log1p-transformed.\n",
    "    gene_list : list\n",
    "        Disease gene list of length n_disease_gene.\n",
    "    gene_weight : array_like, default=None\n",
    "        Gene weights of length n_disease_gene for genes in the gene_list.\n",
    "        If gene_weight=None, the weights are set to be one.\n",
    "    ctrl_match_key : str, default=\"mean_var\"\n",
    "        Gene-level statistic used for matching control and disease genes;\n",
    "        should be in `data.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"]`.\n",
    "    n_ctrl : int, default=1000\n",
    "        Number of control gene sets.\n",
    "    n_genebin : int, default=200\n",
    "        Number of bins for dividing genes by ctrl_match_key if\n",
    "        `data.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"][ctrl_match_key]` is a continuous variable.\n",
    "    weight_opt : str, default=\"vs\"\n",
    "        Option for computing the raw score\n",
    "\n",
    "        - 'uniform': average over the genes in the gene_list.\n",
    "        - 'vs': weighted average with weights equal to 1/sqrt(technical_variance_of_logct).\n",
    "        - 'inv_std': weighted average with weights equal to 1/std.\n",
    "        - 'od': overdispersion score.\n",
    "\n",
    "    copy : bool, default=False\n",
    "        If to make copy of the AnnData object to avoid writing on the orignal data.\n",
    "    return_raw_ctrl_score : bool, default=False\n",
    "        If to return raw control scores.\n",
    "    return_norm_ctrl_score : bool, default=False\n",
    "        If to return normalized control scores.\n",
    "    random_seed : int, default=0\n",
    "        Random seed.\n",
    "    verbose : bool, default=False\n",
    "        If to output messages.\n",
    "    save_intermediate : str, default=None\n",
    "        File path prefix for saving intermediate results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_res : pandas.DataFrame (dtype=np.float32)\n",
    "        scDRS results of shape (n_cell, n_key) with columns\n",
    "\n",
    "        - raw_score: raw disease scores.\n",
    "        - norm_score: normalized disease scores.\n",
    "        - mc_pval: Monte Carlo p-values based on the normalized control scores of the same cell.\n",
    "        - pval: scDRS individual cell-level disease-association p-values.\n",
    "        - nlog10_pval: -log10(pval). Needed in case the single precision (np.float32) gives inaccurate p-values\n",
    "        - zscore: one-side z-score converted from pval.\n",
    "        - ctrl_raw_score_*: raw control scores.\n",
    "        - ctrl_norm_score_*: normalized control scores.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    adata = data.copy() if copy else data\n",
    "    if sp.sparse.issparse(adata.X) and not isinstance(adata.X, sp.sparse.csc_matrix):\n",
    "        adata.X = sp.sparse.csc_matrix(adata.X)\n",
    "    n_cell, n_gene = adata.shape\n",
    "\n",
    "    # Check preprocessing information\n",
    "    assert (\n",
    "        \"SCDRS_PARAM\" in adata.uns\n",
    "    ), \"adata.uns['SCDRS_PARAM'] not found, run `scdrs.pp.preprocess` first\"\n",
    "\n",
    "    # Check GENE_STATS from adata.uns[\"SCDRS_PARAM\"]\n",
    "    assert (\n",
    "        \"GENE_STATS\" in adata.uns[\"SCDRS_PARAM\"]\n",
    "    ), \"adata.uns['SCDRS_PARAM']['GENE_STATS'] not found, run `scdrs.pp.preprocess` first\"\n",
    "\n",
    "    gene_stats_set_expect = {\"mean\", \"var\", \"var_tech\"}\n",
    "    gene_stats_set = set(adata.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"])\n",
    "    assert (\n",
    "        len(gene_stats_set_expect - gene_stats_set) == 0\n",
    "    ), \"One of 'mean', 'var', 'var_tech' not found in adata.uns['SCDRS_PARAM']['GENE_STATS'], run `scdrs.pp.preprocess` first\"\n",
    "\n",
    "    # Check if ctrl_match_key is in GENE_STATS\n",
    "    assert ctrl_match_key in adata.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"], (\n",
    "        \"ctrl_match_key=%s not found in adata.uns['SCDRS_PARAM']['GENE_STATS']\"\n",
    "        % ctrl_match_key\n",
    "    )\n",
    "\n",
    "    # Check if weight_opt is legal\n",
    "    assert weight_opt in {\"uniform\", \"vs\", \"inv_std\", \"od\"}, (\n",
    "        \"weight_opt=%s is not one of {'uniform', 'vs', 'inv_std', 'od}'\" % weight_opt\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        msg = \"# scdrs.method.score_cell summary:\"\n",
    "        msg += \"\\n    n_cell=%d, n_gene=%d,\" % (n_cell, n_gene)\n",
    "        msg += \"\\n    n_disease_gene=%d,\" % len(gene_list)\n",
    "        msg += \"\\n    n_ctrl=%d, n_genebin=%d,\" % (n_ctrl, n_genebin)\n",
    "        msg += \"\\n    ctrl_match_key='%s',\" % ctrl_match_key\n",
    "        msg += \"\\n    weight_opt='%s',\" % weight_opt\n",
    "        msg += \"\\n    return_ctrl_raw_score=%s,\" % return_ctrl_raw_score\n",
    "        msg += \"\\n    return_ctrl_norm_score=%s,\" % return_ctrl_norm_score\n",
    "        msg += \"\\n    random_seed=%d, verbose=%s,\" % (random_seed, verbose)\n",
    "        msg += \"\\n    save_intermediate=%s,\" % save_intermediate\n",
    "        print(msg)\n",
    "\n",
    "    # Load parameters\n",
    "    flag_sparse = adata.uns[\"SCDRS_PARAM\"][\"FLAG_SPARSE\"]\n",
    "    flag_cov = adata.uns[\"SCDRS_PARAM\"][\"FLAG_COV\"]\n",
    "\n",
    "    df_gene = adata.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"].loc[adata.var_names].copy()\n",
    "    df_gene[\"gene\"] = df_gene.index\n",
    "    df_gene.drop_duplicates(subset=\"gene\", inplace=True)\n",
    "\n",
    "    gene_list = list(gene_list)\n",
    "    if gene_weight is not None:\n",
    "        gene_weight = list(gene_weight)\n",
    "    else:\n",
    "        gene_weight = [1] * len(gene_list)\n",
    "\n",
    "    # Overlap gene_list with df_gene[\"gene\"]\n",
    "    dic_gene_weight = {x: y for x, y in zip(gene_list, gene_weight)}\n",
    "    gene_list = sorted(set(gene_list) & set(df_gene[\"gene\"]))\n",
    "    gene_weight = [dic_gene_weight[x] for x in gene_list]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"# scdrs.method.score_cell: use %d overlapping genes for scoring\"\n",
    "            % len(gene_list)\n",
    "        )\n",
    "\n",
    "    # Select control gene sets\n",
    "    dic_ctrl_list, dic_ctrl_weight = _select_ctrl_geneset(\n",
    "        df_gene, gene_list, gene_weight, ctrl_match_key, n_ctrl, n_genebin, random_seed\n",
    "    )\n",
    "    # Compute raw scores\n",
    "    v_raw_score, v_score_weight = _compute_raw_score(\n",
    "        adata, gene_list, gene_weight, weight_opt\n",
    "    )\n",
    "\n",
    "    mat_ctrl_raw_score = np.zeros([n_cell, n_ctrl])\n",
    "    mat_ctrl_weight = np.zeros([len(gene_list), n_ctrl])\n",
    "        \n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(_compute_raw_score)(adata, dic_ctrl_list[i_ctrl], dic_ctrl_weight[i_ctrl], weight_opt) \n",
    "        for i_ctrl in range(n_ctrl)\n",
    "    )\n",
    "    \n",
    "    for i_ctrl, (v_ctrl_raw_score, v_ctrl_weight) in enumerate(results):\n",
    "        mat_ctrl_raw_score[:, i_ctrl] = v_ctrl_raw_score\n",
    "        mat_ctrl_weight[:, i_ctrl] = v_ctrl_weight\n",
    "\n",
    "    # Compute normalized scores\n",
    "    v_var_ratio_c2t = np.ones(n_ctrl)\n",
    "    if (ctrl_match_key == \"mean_var\") & (weight_opt in [\"uniform\", \"vs\", \"inv_std\"]):\n",
    "        # For mean_var matched control genes and raw scores computed as weighted average,\n",
    "        # estimate variance ratio assuming independence.\n",
    "        for i_ctrl in range(n_ctrl):\n",
    "            v_var_ratio_c2t[i_ctrl] = (\n",
    "                df_gene.loc[dic_ctrl_list[i_ctrl], \"var\"]\n",
    "                * mat_ctrl_weight[:, i_ctrl] ** 2\n",
    "            ).sum()\n",
    "        v_var_ratio_c2t /= (df_gene.loc[gene_list, \"var\"] * v_score_weight ** 2).sum()\n",
    "\n",
    "    v_norm_score, mat_ctrl_norm_score = _correct_background(\n",
    "        v_raw_score,\n",
    "        mat_ctrl_raw_score,\n",
    "        v_var_ratio_c2t,\n",
    "        save_intermediate=save_intermediate,\n",
    "    )\n",
    "\n",
    "    # Get p-values\n",
    "    mc_p = (1 + (mat_ctrl_norm_score.T >= v_norm_score).sum(axis=0)) / (1 + n_ctrl)\n",
    "    pooled_p = _get_p_from_empi_null(v_norm_score, mat_ctrl_norm_score.flatten())\n",
    "    nlog10_pooled_p = -np.log10(pooled_p)\n",
    "    pooled_z = -sp.stats.norm.ppf(pooled_p).clip(min=-10, max=10)\n",
    "\n",
    "    # Return result\n",
    "    dic_res = {\n",
    "        \"raw_score\": v_raw_score,\n",
    "        \"norm_score\": v_norm_score,\n",
    "        \"mc_pval\": mc_p,\n",
    "        \"pval\": pooled_p,\n",
    "        \"nlog10_pval\": nlog10_pooled_p,\n",
    "        \"zscore\": pooled_z,\n",
    "    }\n",
    "    if return_ctrl_raw_score:\n",
    "        for i in range(n_ctrl):\n",
    "            dic_res[\"ctrl_raw_score_%d\" % i] = mat_ctrl_raw_score[:, i]\n",
    "    if return_ctrl_norm_score:\n",
    "        for i in range(n_ctrl):\n",
    "            dic_res[\"ctrl_norm_score_%d\" % i] = mat_ctrl_norm_score[:, i]\n",
    "    df_res = pd.DataFrame(index=adata.obs.index, data=dic_res, dtype=np.float32)\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def _select_ctrl_geneset(\n",
    "    input_df_gene,\n",
    "    gene_list,\n",
    "    gene_weight,\n",
    "    ctrl_match_key,\n",
    "    n_ctrl,\n",
    "    n_genebin,\n",
    "    random_seed,\n",
    "):\n",
    "\n",
    "    \"\"\"Subroutine for `scdrs.method.score_cell`. Select control gene sets that match\n",
    "    the disease gene set by `ctrl_match_key`.\n",
    "\n",
    "    It recognizes `ctrl_match_key` as categorical if the number of unique values is\n",
    "    less than 10% of the total number of values, and otherwise continuous. For\n",
    "    categorical `ctrl_match_key`, genes are matched within each category. For continuous\n",
    "    `ctrl_match_key`, genes are divided into `n_genebin` bins and are matched within\n",
    "    each bin. A matched control gene takes the same weight as the disease gene,\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    input_df_gene : pd.DataFrame\n",
    "        Gene-level statistics of shape (n_gene, n_stats).\n",
    "    gene_list : list\n",
    "        Disease gene list of length n_disease_gene.\n",
    "    gene_weight : list\n",
    "        Gene weights of length n_disease_gene for genes in the gene_list.\n",
    "    ctrl_match_key : str\n",
    "        Gene-level statistic used for matching control and disease genes;\n",
    "        should be in `input_df_gene`.\n",
    "    n_ctrl : int\n",
    "        Number of control gene sets.\n",
    "    n_genebin : int\n",
    "        Number of bins for dividing genes by ctrl_match_key if\n",
    "        `input_df_gene[ctrl_match_key]` is a continuous variable.\n",
    "    random_seed : int\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dic_ctrl_list : dict of lists\n",
    "        dic_ctrl_list[i]: the i-th control gene list\n",
    "    dic_ctrl_weight : dict of lists\n",
    "        dic_ctrl_weight[i]: weights for the i-th control gene list\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    df_gene = input_df_gene.copy()\n",
    "    if \"gene\" not in df_gene:\n",
    "        df_gene[\"gene\"] = df_gene.index\n",
    "    disease_gene_set = set(gene_list)\n",
    "    dic_gene_weight = {x: y for x, y in zip(gene_list, gene_weight)}\n",
    "\n",
    "    # Divide genes into equal-sized bins based on ctrl_match_key\n",
    "    if df_gene[ctrl_match_key].unique().shape[0] < df_gene.shape[0] / 10:\n",
    "        df_gene_bin = df_gene.groupby(ctrl_match_key).agg({\"gene\": set})\n",
    "    else:\n",
    "        df_gene[\"qbin\"] = pd.qcut(\n",
    "            df_gene[ctrl_match_key], q=n_genebin, labels=False, duplicates=\"drop\"\n",
    "        )\n",
    "        df_gene_bin = df_gene.groupby(\"qbin\").agg({\"gene\": set})\n",
    "\n",
    "    # Find ctrl_match_key matched control genes\n",
    "    dic_ctrl_list = {x: [] for x in range(n_ctrl)}\n",
    "    dic_ctrl_weight = {x: [] for x in range(n_ctrl)}\n",
    "    for bin_ in df_gene_bin.index:\n",
    "        bin_gene = sorted(df_gene_bin.loc[bin_, \"gene\"])\n",
    "        bin_disease_gene = sorted(df_gene_bin.loc[bin_, \"gene\"] & disease_gene_set)\n",
    "        if len(bin_disease_gene) > 0:\n",
    "            for i_list in np.arange(n_ctrl):\n",
    "                dic_ctrl_list[i_list].extend(\n",
    "                    np.random.choice(\n",
    "                        bin_gene, size=len(bin_disease_gene), replace=False\n",
    "                    )\n",
    "                )\n",
    "                dic_ctrl_weight[i_list].extend(\n",
    "                    [dic_gene_weight[x] for x in bin_disease_gene]\n",
    "                )\n",
    "\n",
    "    return dic_ctrl_list, dic_ctrl_weight\n",
    "\n",
    "\n",
    "def _compute_raw_score(adata, gene_list, gene_weight, weight_opt):\n",
    "    \"\"\"Compute raw score\n",
    "        v_score_weight = gene_weight * {uniform/vs/inv_std}\n",
    "        `SCDRS_PARAM` is assumed to have been computed using `sparse_reg_out`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        Single-cell data of shape (n_cell, n_gene). Assumed\n",
    "        to be size-factor-normalized and log1p-transformed.\n",
    "    gene_list : list\n",
    "        Disease gene list of length n_disease_gene.\n",
    "    gene_weight : list\n",
    "        Gene weights of length n_disease_gene for genes in the gene_list.\n",
    "    weight_opt : str\n",
    "        Option for computing the raw score\n",
    "        - 'uniform': average over the genes in the gene_list.\n",
    "        - 'vs': weighted average with weights equal to 1/sqrt(technical_variance_of_logct).\n",
    "        - 'inv_std': weighted average with weights equal to 1/std.\n",
    "        - 'od': overdispersion score.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v_raw_score : np.ndarray\n",
    "        Raw score of shape (n_cell,).\n",
    "    v_score_weight : np.ndarray\n",
    "        Gene weights of shape (n_disease_gene,).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    gene_list = list(gene_list)\n",
    "    gene_weight = list(gene_weight)\n",
    "\n",
    "    assert weight_opt in {\"uniform\", \"vs\", \"inv_std\", \"od\"}, (\n",
    "        \"weight_opt=%s is not one of {'uniform', 'vs', 'inv_std', 'od}'\" % weight_opt\n",
    "    )\n",
    "\n",
    "    # Compute overdispersion score\n",
    "    # (used only for benchmarking, do not support implicit covariate correction mode)\n",
    "    if weight_opt == \"od\":\n",
    "        return _compute_overdispersion_score(adata, gene_list, gene_weight)\n",
    "\n",
    "    # Compute other weighted average scores\n",
    "    assert (\n",
    "        \"SCDRS_PARAM\" in adata.uns\n",
    "    ), \"adata.uns['SCDRS_PARAM'] not found, run `scdrs.pp.preprocess` first\"\n",
    "\n",
    "    df_gene = adata.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"]\n",
    "    flag_sparse = adata.uns[\"SCDRS_PARAM\"][\"FLAG_SPARSE\"]\n",
    "    flag_cov = adata.uns[\"SCDRS_PARAM\"][\"FLAG_COV\"]\n",
    "\n",
    "    if weight_opt == \"uniform\":\n",
    "        v_score_weight = np.ones(len(gene_list))\n",
    "    if weight_opt == \"vs\":\n",
    "        v_score_weight = 1 / np.sqrt(df_gene.loc[gene_list, \"var_tech\"].values + 1e-2)\n",
    "    if weight_opt == \"inv_std\":\n",
    "        v_score_weight = 1 / np.sqrt(df_gene.loc[gene_list, \"var\"].values + 1e-2)\n",
    "\n",
    "    if gene_weight is not None:\n",
    "        v_score_weight = v_score_weight * np.array(gene_weight)\n",
    "    v_score_weight = v_score_weight / v_score_weight.sum()\n",
    "\n",
    "    if flag_sparse and flag_cov:\n",
    "        # Implicit covariate correction mode\n",
    "        cell_list = list(adata.obs_names)\n",
    "        cov_list = list(adata.uns[\"SCDRS_PARAM\"][\"COV_MAT\"])\n",
    "        cov_mat = adata.uns[\"SCDRS_PARAM\"][\"COV_MAT\"].loc[cell_list, cov_list].values\n",
    "        cov_beta = (\n",
    "            adata.uns[\"SCDRS_PARAM\"][\"COV_BETA\"].loc[gene_list, cov_list].values.T\n",
    "        )\n",
    "        gene_mean = adata.uns[\"SCDRS_PARAM\"][\"COV_GENE_MEAN\"].loc[gene_list].values\n",
    "\n",
    "        # Compute v_raw_score = transformed_X @ v_score_weight\n",
    "        # where transformed_X = adata.X + cov_mat @ cov_beta + gene_mean\n",
    "        gene_idx = adata.var_names.get_indexer(gene_list)\n",
    "        gene_idx = gene_idx[gene_idx >= 0]\n",
    "        v_raw_score = (\n",
    "            adata.X[:,gene_idx].dot(v_score_weight)\n",
    "            + cov_mat @ (cov_beta @ v_score_weight)\n",
    "            + gene_mean @ v_score_weight\n",
    "        ).flatten()\n",
    "    else:\n",
    "        # Normal mode\n",
    "        gene_idx = adata.var_names.get_indexer(gene_list)\n",
    "        gene_idx = gene_idx[gene_idx >= 0]\n",
    "        v_raw_score = adata.X[:,gene_idx].dot(v_score_weight).reshape([-1])\n",
    "\n",
    "    return v_raw_score, v_score_weight\n",
    "\n",
    "\n",
    "def _compute_overdispersion_score(adata, gene_list, gene_weight):\n",
    "    \"\"\"Compute overdispersion score\n",
    "\n",
    "        Raw weight: w_g_raw = gene_weight / \\sigma_{tech,g}^2\n",
    "\n",
    "        Normalized weight: w_g = w_g_raw / \\sum_g w_g_raw\n",
    "\n",
    "        Overdispersion score: s_c = \\sum_g w_g * [(X_cg - \\mu_g)^2 - \\sigma_{tech,g}^2]\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    adata : anndata.AnnData\n",
    "        Single-cell data of shape (n_cell, n_gene). Assumed\n",
    "        to be size-factor-normalized and log1p-transformed.\n",
    "    gene_list : list\n",
    "        Disease gene list of length n_disease_gene.\n",
    "    gene_weight : list\n",
    "        Gene weights of length n_disease_gene for genes in the gene_list.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v_raw_score : np.ndarray\n",
    "        Raw score of shape (n_cell,).\n",
    "    v_score_weight : np.ndarray\n",
    "        Gene weights of shape (n_disease_gene,).\n",
    "    \"\"\"\n",
    "\n",
    "    gene_list = list(gene_list)\n",
    "    gene_weight = list(gene_weight)\n",
    "\n",
    "    assert (\n",
    "        \"SCDRS_PARAM\" in adata.uns\n",
    "    ), \"adata.uns['SCDRS_PARAM'] not found, run `scdrs.pp.preprocess` first\"\n",
    "\n",
    "    # Mode check\n",
    "    flag_sparse = adata.uns[\"SCDRS_PARAM\"][\"FLAG_SPARSE\"]\n",
    "    flag_cov = adata.uns[\"SCDRS_PARAM\"][\"FLAG_COV\"]\n",
    "    gene_idx = adata.var_names.get_indexer(gene_list)\n",
    "    gene_idx = gene_idx[gene_idx >= 0]\n",
    "    if flag_sparse and flag_cov:\n",
    "        cell_list = list(adata.obs_names)\n",
    "        cov_list = list(adata.uns[\"SCDRS_PARAM\"][\"COV_MAT\"])\n",
    "        mat_X = (\n",
    "            adata.X[:,gene_idx].toarray()\n",
    "            + adata.uns[\"SCDRS_PARAM\"][\"COV_MAT\"]\n",
    "            .loc[cell_list, cov_list]\n",
    "            .values.dot(\n",
    "                adata.uns[\"SCDRS_PARAM\"][\"COV_BETA\"].loc[gene_list, cov_list].values.T\n",
    "            )\n",
    "            + adata.uns[\"SCDRS_PARAM\"][\"COV_GENE_MEAN\"].loc[gene_list].values\n",
    "        )\n",
    "    else:\n",
    "        mat_X = adata.X[:,gene_idx]\n",
    "\n",
    "    v_mean = adata.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"].loc[gene_list, \"mean\"].values\n",
    "    v_var_tech = (\n",
    "        adata.uns[\"SCDRS_PARAM\"][\"GENE_STATS\"].loc[gene_list, \"var_tech\"].values\n",
    "    )\n",
    "\n",
    "    v_w = 1 / (v_var_tech + 1e-2)\n",
    "    if gene_weight is not None:\n",
    "        v_w = v_w * np.array(gene_weight)\n",
    "    v_w = v_w / v_w.sum()\n",
    "\n",
    "    # Compute overdispersion score\n",
    "    if sp.sparse.issparse(mat_X):\n",
    "        v_raw_score = mat_X.power(2).dot(v_w).reshape([-1])  # Quadratic term\n",
    "    else:\n",
    "        v_raw_score = (mat_X ** 2).dot(v_w).reshape([-1])  # Quadratic term\n",
    "    v_raw_score = v_raw_score - mat_X.dot(2 * v_w * v_mean).reshape([-1])  # Linear term\n",
    "    v_raw_score = (\n",
    "        v_raw_score + (v_w * (v_mean ** 2 - v_var_tech)).sum()\n",
    "    )  # Constant term\n",
    "\n",
    "    return v_raw_score, np.ones(len(gene_list))\n",
    "\n",
    "\n",
    "def _correct_background(\n",
    "    v_raw_score, mat_ctrl_raw_score, v_var_ratio_c2t, save_intermediate=None\n",
    "):\n",
    "    \"\"\"Cell-wise and gene-wise background correction\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    v_raw_score : np.ndarray\n",
    "        Disease raw score of shape (n_cell,).\n",
    "    mat_ctrl_raw_score : np.ndarray\n",
    "        Disease raw control scores of shape (n_cell,n_ctrl).\n",
    "    v_var_ratio_c2t : np.ndarray\n",
    "        Ratio of independent variance between control scores and disease score,\n",
    "        of shape (n_ctrl).\n",
    "    save_intermediate : str\n",
    "        File path prefix for saving intermediate results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v_norm_score : np.ndarray\n",
    "        Normalized disease score of shape (n_cell,)\n",
    "    mat_ctrl_norm_score : np.ndarray\n",
    "        Normalized control scores of shape (n_cell,n_ctrl).\n",
    "    \"\"\"\n",
    "\n",
    "    if save_intermediate is not None:\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".raw_score.tsv.gz\",\n",
    "            v_raw_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".ctrl_raw_score.tsv.gz\",\n",
    "            mat_ctrl_raw_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "\n",
    "    # Zero-values are assigned the smallest values at the end\n",
    "    ind_zero_score = v_raw_score == 0\n",
    "    ind_zero_ctrl_score = mat_ctrl_raw_score == 0\n",
    "\n",
    "    # First gene set alignment: mean 0 and same independent variance\n",
    "    v_raw_score = v_raw_score - v_raw_score.mean()\n",
    "    mat_ctrl_raw_score = mat_ctrl_raw_score - mat_ctrl_raw_score.mean(axis=0)\n",
    "    mat_ctrl_raw_score = mat_ctrl_raw_score / np.sqrt(v_var_ratio_c2t)\n",
    "    if save_intermediate is not None:\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".raw_score.1st_gs_alignment.tsv.gz\",\n",
    "            v_raw_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".ctrl_raw_score.1st_gs_alignment.tsv.gz\",\n",
    "            mat_ctrl_raw_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "\n",
    "    # Cell-wise standardization\n",
    "    v_mean = mat_ctrl_raw_score.mean(axis=1)\n",
    "    v_std = mat_ctrl_raw_score.std(axis=1)\n",
    "    v_norm_score = v_raw_score.copy()\n",
    "    v_norm_score = (v_norm_score - v_mean) / v_std\n",
    "    mat_ctrl_norm_score = ((mat_ctrl_raw_score.T - v_mean) / v_std).T\n",
    "    if save_intermediate is not None:\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".raw_score.cellwise_standardization.tsv.gz\",\n",
    "            v_norm_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".ctrl_raw_score.cellwise_standardization.tsv.gz\",\n",
    "            mat_ctrl_norm_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "\n",
    "    # Second gene set alignment: mean 0\n",
    "    v_norm_score = v_norm_score - v_norm_score.mean()\n",
    "    mat_ctrl_norm_score = mat_ctrl_norm_score - mat_ctrl_norm_score.mean(axis=0)\n",
    "    if save_intermediate is not None:\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".raw_score.2nd_gs_alignment.tsv.gz\",\n",
    "            v_norm_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".ctrl_raw_score.2nd_gs_alignment.tsv.gz\",\n",
    "            mat_ctrl_norm_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "\n",
    "    # Set cells with raw_score=0 to the minimum norm_score value\n",
    "    norm_score_min = min(v_norm_score.min(), mat_ctrl_norm_score.min())\n",
    "    v_norm_score[ind_zero_score] = norm_score_min - 1e-3\n",
    "    mat_ctrl_norm_score[ind_zero_ctrl_score] = norm_score_min\n",
    "    if save_intermediate is not None:\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".raw_score.final.tsv.gz\",\n",
    "            v_norm_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "        np.savetxt(\n",
    "            save_intermediate + \".ctrl_raw_score.final.tsv.gz\",\n",
    "            mat_ctrl_norm_score,\n",
    "            fmt=\"%.9e\",\n",
    "            delimiter=\"\\t\",\n",
    "        )\n",
    "\n",
    "    return v_norm_score, mat_ctrl_norm_score\n",
    "\n",
    "\n",
    "def _get_p_from_empi_null(v_t, v_t_null):\n",
    "    \"\"\"Compute p-value from empirical null\n",
    "    For score T and a set of null score T_1,...T_N, the p-value is\n",
    "\n",
    "        p= [1 + \\Sigma_{i=1}^N 1_{ (T_i \\geq T) }] / (1+N)\n",
    "\n",
    "    If T, T_1, ..., T_N are i.i.d. variables following a null distritbuion,\n",
    "    then p is super-uniform.\n",
    "\n",
    "    The naive algorithm is N^2. Here we provide an O(N log N) algorithm to\n",
    "    compute the p-value for each of the N elements in v_t\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    v_t : np.ndarray\n",
    "        Observed score of shape (M,).\n",
    "    v_t_null : np.ndarray\n",
    "        Null scores of shape (N,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v_p: : np.ndarray\n",
    "        P-value for each element in v_t of shape (M,).\n",
    "    \"\"\"\n",
    "\n",
    "    v_t = np.array(v_t)\n",
    "    v_t_null = np.array(v_t_null)\n",
    "\n",
    "    v_t_null = np.sort(v_t_null)\n",
    "    v_pos = np.searchsorted(v_t_null, v_t, side=\"left\")\n",
    "    v_p = (v_t_null.shape[0] - v_pos + 1) / (v_t_null.shape[0] + 1)\n",
    "    return v_p\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "######################### Code for comparison methods ########################\n",
    "##############################################################################\n",
    "def score_cell_vision(adata, gene_list):\n",
    "\n",
    "    \"\"\"Score cells based on the trait gene set\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    data (n_cell, n_gene) : AnnData\n",
    "        data.X should contain size-normalized log1p transformed count data\n",
    "    gene_list (n_disease_gene) : list\n",
    "        Trait gene list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_res (n_cell, n_key) : pd.DataFrame (dtype=np.float32)\n",
    "        Columns:\n",
    "        1. score: Vision signature score\n",
    "        2. pval: p-value computed from the Vision score\n",
    "    \"\"\"\n",
    "\n",
    "    gene_list = sorted(set(gene_list) & set(adata.var_names))\n",
    "    v_mean, v_var = scdrs.pp._get_mean_var(adata.X, axis=1)\n",
    "\n",
    "    v_score = adata[:, gene_list].X.mean(axis=1)\n",
    "    v_score = np.array(v_score).reshape([-1])\n",
    "    v_score = (v_score - v_mean) / np.sqrt(v_var / len(gene_list))\n",
    "    v_p = 1 - sp.stats.norm.cdf(v_score)\n",
    "\n",
    "    v_norm_score = (v_score - v_score.mean()) / v_score.std()\n",
    "    v_norm_p = 1 - sp.stats.norm.cdf(v_norm_score)\n",
    "\n",
    "    dic_res = {\n",
    "        \"score\": v_score,\n",
    "        \"pval\": v_p,\n",
    "        \"norm_score\": v_norm_score,\n",
    "        \"norm_pval\": v_norm_p,\n",
    "    }\n",
    "    df_res = pd.DataFrame(index=adata.obs.index, data=dic_res, dtype=np.float32)\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def score_cell_scanpy(adata, gene_list):\n",
    "\n",
    "    \"\"\"Score cells based on the trait gene set\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "        data (n_cell, n_gene) : AnnData\n",
    "            data.X should contain size-normalized log1p transformed count data\n",
    "        gene_list (n_disease_gene) : list\n",
    "            Trait gene list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        df_res (n_cell, n_key) : pd.DataFrame (dtype=np.float32)\n",
    "            Columns:\n",
    "            1. score: Vision signature score\n",
    "            2. pval: p-value computed from the Vision score\n",
    "    \"\"\"\n",
    "\n",
    "    gene_list = sorted(set(gene_list) & set(adata.var_names))\n",
    "    sc.tl.score_genes(adata, gene_list=gene_list)\n",
    "\n",
    "    v_score = adata.obs[\"score\"]\n",
    "    v_score_z = (v_score - v_score.mean()) / np.sqrt(v_score.var())\n",
    "    v_p = 1 - sp.stats.norm.cdf(v_score_z)\n",
    "\n",
    "    dic_res = {\"score\": v_score, \"score_z\": v_score_z, \"pval\": v_p}\n",
    "    df_res = pd.DataFrame(index=adata.obs.index, data=dic_res, dtype=np.float32)\n",
    "    return df_res\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "######################## Code for downstream analysis ########################\n",
    "##############################################################################\n",
    "def downstream_group_analysis(\n",
    "    adata: anndata.AnnData,\n",
    "    df_full_score: pd.DataFrame,\n",
    "    group_cols: List[str],\n",
    "    fdr_thresholds: List[float] = [0.05, 0.1, 0.2],\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    scDRS group-level analysis.\n",
    "\n",
    "    For each annotation in `group_cols` and each group of cells in the annotation, compute:\n",
    "\n",
    "    1. Proportion of FDR < 0.1 cells.\n",
    "    2. Group-level trait association.\n",
    "    3. Group-level heterogeneity.\n",
    "\n",
    "    `connectivities` is expected in `adata.obsp` for the group-level heterogeneity analysis.\n",
    "    Recommended parameters: `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=20)`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        Single-cell data of shape (n_cell, n_gene). Assumed\n",
    "        to be size-factor-normalized and log1p-transformed.\n",
    "    df_full_score : pd.DataFrame\n",
    "        scDRS `.full_score` file for a given trait.\n",
    "    group_cols : list of str\n",
    "        List of column names in adata.obs used to define cell groups.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict_df_res : Dict[str, pd.DataFrame]\n",
    "        Group-level statistics (n_group, n_stats) keyed by the group names.\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        \"connectivities\" in adata.obsp\n",
    "    ), \"Expect `connectivities` in `adata.obsp`; run `sc.pp.neighbors` first\"\n",
    "\n",
    "    assert (\n",
    "        len(set(group_cols) - set(adata.obs)) == 0\n",
    "    ), \"Missing `group_cols` variables from `adata.obs.columns`.\"\n",
    "\n",
    "    # Align cells between `adata` and `df_full_score`.\n",
    "    cell_list = sorted(set(adata.obs_names) & set(df_full_score.index))\n",
    "    control_list = [x for x in df_full_score.columns if x.startswith(\"ctrl_norm_score\")]\n",
    "    n_ctrl = len(control_list)\n",
    "    df_reg = adata.obs.loc[cell_list, group_cols].copy()\n",
    "    df_reg = df_reg.join(\n",
    "        df_full_score.loc[cell_list, [\"norm_score\"] + control_list + [\"pval\"]]\n",
    "    )\n",
    "\n",
    "    # Group-level analysis; dict_df_res : group_col -> df_res\n",
    "    dict_df_res = {}\n",
    "    for group_col in group_cols:\n",
    "        group_list = sorted(set(adata.obs[group_col]))\n",
    "        res_cols = [\n",
    "            \"n_cell\",\n",
    "            \"n_ctrl\",\n",
    "            \"assoc_mcp\",\n",
    "            \"assoc_mcz\",\n",
    "            \"hetero_mcp\",\n",
    "            \"hetero_mcz\",\n",
    "        ]\n",
    "        for fdr_threshold in fdr_thresholds:\n",
    "            res_cols.append(f\"n_fdr_{fdr_threshold}\")\n",
    "\n",
    "        df_res = pd.DataFrame(index=group_list, columns=res_cols, dtype=np.float32)\n",
    "        df_res.index.name = \"group\"\n",
    "\n",
    "        df_fdr = pd.DataFrame(\n",
    "            {\"fdr\": multipletests(df_reg[\"pval\"].values, method=\"fdr_bh\")[1]},\n",
    "            index=df_reg.index,\n",
    "        )\n",
    "\n",
    "        for group in group_list:\n",
    "            group_cell_list = list(df_reg.index[df_reg[group_col] == group])\n",
    "            # Basic info\n",
    "            df_res.loc[group, [\"n_cell\", \"n_ctrl\"]] = [len(group_cell_list), n_ctrl]\n",
    "\n",
    "            # Number of FDR < fdr_threshold cells in each group\n",
    "            for fdr_threshold in fdr_thresholds:\n",
    "                df_res.loc[group, f\"n_fdr_{fdr_threshold}\"] = (\n",
    "                    df_fdr.loc[group_cell_list, \"fdr\"].values < fdr_threshold\n",
    "                ).sum()\n",
    "\n",
    "        # Association\n",
    "        for group in group_list:\n",
    "            group_cell_list = list(df_reg.index[df_reg[group_col] == group])\n",
    "            score_q95 = np.quantile(df_reg.loc[group_cell_list, \"norm_score\"], 0.95)\n",
    "            v_ctrl_score_q95 = np.quantile(\n",
    "                df_reg.loc[group_cell_list, control_list], 0.95, axis=0\n",
    "            )\n",
    "            mc_p = ((v_ctrl_score_q95 >= score_q95).sum() + 1) / (\n",
    "                v_ctrl_score_q95.shape[0] + 1\n",
    "            )\n",
    "            mc_z = (score_q95 - v_ctrl_score_q95.mean()) / v_ctrl_score_q95.std()\n",
    "            df_res.loc[group, [\"assoc_mcp\", \"assoc_mcz\"]] = [mc_p, mc_z]\n",
    "\n",
    "        # Heterogeneity\n",
    "        df_rls = test_gearysc(\n",
    "            adata[cell_list], df_reg.loc[cell_list, :], groupby=group_col\n",
    "        )\n",
    "        for ct in group_list:\n",
    "            mc_p, mc_z = df_rls.loc[ct, [\"pval\", \"zsc\"]]\n",
    "            df_res.loc[ct, [\"hetero_mcp\", \"hetero_mcz\"]] = [mc_p, mc_z]\n",
    "\n",
    "        # write to dict for this group_col\n",
    "        dict_df_res[group_col] = df_res\n",
    "\n",
    "    return dict_df_res\n",
    "\n",
    "\n",
    "def downstream_corr_analysis(\n",
    "    adata: anndata.AnnData, df_full_score: pd.DataFrame, var_cols: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    scDRS cell-level correlation analysis.\n",
    "\n",
    "    For a given individual cell-level annotation (e.g., T cell effectorness gradient),\n",
    "    assess association between disease and the individual cell-level variable\n",
    "    (control-score-based Monte Carlo tests using Pearson's correlation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        Single-cell data of shape (n_cell, n_gene). Assumed\n",
    "        to be size-factor-normalized and log1p-transformed.\n",
    "    df_full_score : pd.DataFrame\n",
    "        scDRS `.full_score` file for a given trait.\n",
    "    var_cols : List[str]\n",
    "        List of column names in `adata.obs` for continous cell-level variables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_res : pd.DataFrame\n",
    "        Correlation results (n_var, n_stats).\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        len(set(var_cols) - set(adata.obs)) == 0\n",
    "    ), \"Missing `var_cols` variables from `adata.obs.columns`.\"\n",
    "\n",
    "    cell_list = sorted(set(adata.obs_names) & set(df_full_score.index))\n",
    "    control_list = [x for x in df_full_score.columns if x.startswith(\"ctrl_norm_score\")]\n",
    "    n_ctrl = len(control_list)\n",
    "    df_reg = adata.obs.loc[cell_list, var_cols].copy()\n",
    "    df_reg = df_reg.join(df_full_score.loc[cell_list, [\"norm_score\"] + control_list])\n",
    "\n",
    "    # Variable-disease correlation\n",
    "    col_list = [\"n_ctrl\", \"corr_mcp\", \"corr_mcz\"]\n",
    "    df_res = pd.DataFrame(index=var_cols, columns=col_list, dtype=np.float32)\n",
    "    for var_col in var_cols:\n",
    "        corr_ = np.corrcoef(df_reg[var_col], df_reg[\"norm_score\"])[0, 1]\n",
    "        v_corr_ = np.array(\n",
    "            [np.corrcoef(df_reg[var_col], df_reg[x])[0, 1] for x in control_list]\n",
    "        )\n",
    "        mc_p = ((v_corr_ >= corr_).sum() + 1) / (v_corr_.shape[0] + 1)\n",
    "        mc_z = (corr_ - v_corr_.mean()) / v_corr_.std()\n",
    "        df_res.loc[var_col] = [n_ctrl, mc_p, mc_z]\n",
    "\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def downstream_gene_analysis(\n",
    "    adata: anndata.AnnData, df_full_score: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    scDRS gene-level correlation analysis.\n",
    "\n",
    "    Compute correlation between each gene and the scDRS disease score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        Single-cell data of shape (n_cell, n_gene). Assumed\n",
    "        to be size-factor-normalized and log1p-transformed.\n",
    "    df_full_score : pd.DataFrame\n",
    "        scDRS `.full_score` file for a given trait.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_res : pd.DataFrame\n",
    "        Correlation results (n_gene, n_stats).\n",
    "    \"\"\"\n",
    "\n",
    "    cell_list = sorted(set(adata.obs_names) & set(df_full_score.index))\n",
    "    control_list = [x for x in df_full_score.columns if x.startswith(\"ctrl_norm_score\")]\n",
    "    df_reg = df_full_score.loc[cell_list, [\"norm_score\"]]\n",
    "\n",
    "    mat_expr = adata[cell_list].X.copy()\n",
    "    v_corr = _pearson_corr(mat_expr, df_reg[\"norm_score\"].values)\n",
    "    df_res = pd.DataFrame(\n",
    "        index=adata.var_names, columns=[\"CORR\", \"RANK\"], dtype=np.float32\n",
    "    )\n",
    "    df_res[\"CORR\"] = v_corr\n",
    "    df_res.sort_values(\"CORR\", ascending=False, inplace=True)\n",
    "    df_res[\"RANK\"] = np.arange(df_res.shape[0])\n",
    "    return df_res\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "##################### Subroutines for downstream analysis ####################\n",
    "##############################################################################\n",
    "def test_gearysc(\n",
    "    adata: anndata.AnnData,\n",
    "    df_full_score: pd.DataFrame,\n",
    "    groupby: str,\n",
    "    opt=\"control_distribution_match\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute significance level for Geary's C statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        Must contain `connectivities` to compute the Geary's C statistic.\n",
    "    df_full_score : DataFrame\n",
    "        DataFrame with the scores of the cells, contains\n",
    "        columns `zscore`, `norm_score`, `ctrl_norm_score_{i}`\n",
    "    groupby : str\n",
    "        Column name of the groupby variable.\n",
    "    opt : str\n",
    "        Options:\n",
    "            - \"control_distribution_match\":\n",
    "                The distribution of the scores of the control scores is similar to\n",
    "                the distribution of the scores of the disease scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_rls : DataFrame\n",
    "        DataFrame with the results of the test with `n_group` rows and 4 columns:\n",
    "\n",
    "        - `pval`: significance level of Geary's C\n",
    "        - `trait`: Geary's C test statistic of the trait scores\n",
    "        - `ctrl_mean`: mean of the control scores\n",
    "        - `ctrl_sd`: standard deviation of the control scores\n",
    "    \"\"\"\n",
    "    assert np.all(\n",
    "        df_full_score.index == adata.obs_names\n",
    "    ), \"adata.obs_names must match df_full_score.index\"\n",
    "    norm_score = df_full_score[\"norm_score\"]\n",
    "    ctrl_norm_score = df_full_score[\n",
    "        [col for col in df_full_score.columns if col.startswith(f\"ctrl_norm_score_\")]\n",
    "    ]\n",
    "    n_null = ctrl_norm_score.shape[1]\n",
    "    df_meta = adata.obs.copy()\n",
    "    df_stats = pd.DataFrame(\n",
    "        index=df_meta[groupby].unique(),\n",
    "        columns=[\"trait\"] + [f\"null_{i_null}\" for i_null in range(n_null)],\n",
    "        data=np.nan,\n",
    "    )\n",
    "\n",
    "    for group, df_group in df_meta.groupby(groupby):\n",
    "        group_index = df_group.index\n",
    "        group_adata = adata[group_index]\n",
    "        group_norm_score = norm_score[group_index]\n",
    "        group_ctrl_norm_score = ctrl_norm_score.loc[group_index, :]\n",
    "\n",
    "        if opt == \"control_distribution_match\":\n",
    "            # control distribution match\n",
    "            from scipy.stats import rankdata\n",
    "\n",
    "            def distribution_match(v, ref):\n",
    "                \"\"\"\n",
    "                Use order in `v` to match the distribution of `ref`\n",
    "                \"\"\"\n",
    "                return np.sort(ref)[rankdata(v, method=\"ordinal\") - 1]\n",
    "\n",
    "            df_stats.loc[group, \"trait\"] = gearys_c(\n",
    "                group_adata, group_norm_score.values\n",
    "            )\n",
    "\n",
    "            for i_null in range(n_null):\n",
    "                df_stats.loc[group, f\"null_{i_null}\"] = gearys_c(\n",
    "                    group_adata,\n",
    "                    distribution_match(\n",
    "                        group_ctrl_norm_score.iloc[:, i_null].values,\n",
    "                        ref=group_norm_score,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "        elif opt == \"permutation\":\n",
    "            # permutation\n",
    "            df_stats.loc[group, \"trait\"] = gearys_c(\n",
    "                group_adata, group_norm_score.values\n",
    "            )\n",
    "            for i_null in range(n_null):\n",
    "                df_stats.loc[group, f\"null_{i_null}\"] = gearys_c(\n",
    "                    group_adata, np.random.permutation(group_norm_score.values)\n",
    "                )\n",
    "        elif opt == \"control\":\n",
    "            # control\n",
    "            df_stats.loc[group, \"trait\"] = gearys_c(\n",
    "                group_adata, group_norm_score.values\n",
    "            )\n",
    "            for i_null in range(n_null):\n",
    "                df_stats.loc[group, f\"null_{i_null}\"] = gearys_c(\n",
    "                    group_adata, group_ctrl_norm_score.iloc[:, i_null].values\n",
    "                )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    # Summarize\n",
    "    trait_col = \"trait\"\n",
    "    ctrl_cols = [col for col in df_stats.columns if col.startswith(\"null_\")]\n",
    "    pval = (\n",
    "        (df_stats[trait_col].values > df_stats[ctrl_cols].values.T).sum(axis=0) + 1\n",
    "    ) / (len(ctrl_cols) + 1)\n",
    "    pval[np.isnan(df_stats[trait_col])] = np.nan\n",
    "\n",
    "    df_rls = pd.DataFrame(\n",
    "        {\n",
    "            \"pval\": pval,\n",
    "            \"trait\": df_stats[trait_col].values,\n",
    "            \"ctrl_mean\": df_stats[ctrl_cols].mean(axis=1).values,\n",
    "            \"ctrl_std\": df_stats[ctrl_cols].std(axis=1).values,\n",
    "        },\n",
    "        index=df_stats.index,\n",
    "    )\n",
    "\n",
    "    df_rls[\"zsc\"] = (\n",
    "        -(df_rls[trait_col].values - df_rls[\"ctrl_mean\"]) / df_rls[\"ctrl_std\"]\n",
    "    )\n",
    "    return df_rls\n",
    "\n",
    "\n",
    "def gearys_c(adata, vals):\n",
    "    \"\"\"\n",
    "    Compute Geary's C statistics for an AnnData.\n",
    "\n",
    "    Adopted from https://github.com/ivirshup/scanpy/blob/metrics/scanpy/metrics/_gearys_c.py\n",
    "\n",
    "    :math:`C=\\\\frac{(N - 1)\\\\sum_{i,j} w_{i,j} (x_i - x_j)^2}{2W \\\\sum_i (x_i - \\\\bar{x})^2}`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : AnnData object\n",
    "        adata.obsp[\"Connectivities] should contain the connectivity graph,\n",
    "        with shape (n_obs, n_obs).\n",
    "    vals : array-like\n",
    "        Values to calculate Geary's C for. If one dimensional, should have\n",
    "        shape (n_obs,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    C : float\n",
    "        Geary's C statistics.\n",
    "    \"\"\"\n",
    "    graph = adata.obsp[\"connectivities\"]\n",
    "    assert graph.shape[0] == graph.shape[1]\n",
    "    graph_data = graph.data.astype(np.float64, copy=False)\n",
    "    assert graph.shape[0] == vals.shape[0]\n",
    "    assert np.ndim(vals) == 1\n",
    "\n",
    "    W = graph_data.sum()\n",
    "    N = len(graph.indptr) - 1\n",
    "    vals_bar = vals.mean()\n",
    "    vals = vals.astype(np.float64)\n",
    "\n",
    "    # numerators\n",
    "    total = 0.0\n",
    "    for i in range(N):\n",
    "        s = slice(graph.indptr[i], graph.indptr[i + 1])\n",
    "        # indices of corresponding neighbors\n",
    "        i_indices = graph.indices[s]\n",
    "        # corresponding connecting weights\n",
    "        i_data = graph_data[s]\n",
    "        total += np.sum(i_data * ((vals[i] - vals[i_indices]) ** 2))\n",
    "\n",
    "    numer = (N - 1) * total\n",
    "    denom = 2 * W * ((vals - vals_bar) ** 2).sum()\n",
    "    C = numer / denom\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def _pearson_corr(mat_X, mat_Y):\n",
    "    \"\"\"Pearson's correlation between every columns in mat_X and mat_Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mat_X : np.ndarray\n",
    "        First matrix of shape (N,M1).\n",
    "    mat_Y : np.ndarray\n",
    "        Second matrix of shape (N,M2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mat_corr : np.ndarray\n",
    "        Correlation matrix of shape (M1,M2).\n",
    "    \"\"\"\n",
    "    # If sparse, use _pearson_corr_sparse\n",
    "    if sp.sparse.issparse(mat_X) | sp.sparse.issparse(mat_Y):\n",
    "        return _pearson_corr_sparse(mat_X, mat_Y)\n",
    "\n",
    "    # Reshape\n",
    "    if len(mat_X.shape) == 1:\n",
    "        mat_X = mat_X.reshape([-1, 1])\n",
    "    if len(mat_Y.shape) == 1:\n",
    "        mat_Y = mat_Y.reshape([-1, 1])\n",
    "\n",
    "    mat_X = (mat_X - mat_X.mean(axis=0)) / mat_X.std(axis=0).clip(min=1e-8)\n",
    "    mat_Y = (mat_Y - mat_Y.mean(axis=0)) / mat_Y.std(axis=0).clip(min=1e-8)\n",
    "    mat_corr = mat_X.T.dot(mat_Y) / mat_X.shape[0]\n",
    "    mat_corr = np.array(mat_corr, dtype=np.float32)\n",
    "\n",
    "    if (mat_X.shape[1] == 1) | (mat_Y.shape[1] == 1):\n",
    "        return mat_corr.reshape([-1])\n",
    "    else:\n",
    "        return mat_corr\n",
    "\n",
    "\n",
    "def _pearson_corr_sparse(mat_X, mat_Y):\n",
    "    \"\"\"Pearson's correlation between every columns in mat_X and mat_Y (sparse matrix)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mat_X : np.ndarray\n",
    "        First matrix of shape (N,M1).\n",
    "    mat_Y : np.ndarray\n",
    "        Second matrix of shape (N,M2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mat_corr : np.ndarray\n",
    "        Correlation matrix of shape (M1,M2).\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape\n",
    "    if len(mat_X.shape) == 1:\n",
    "        mat_X = mat_X.reshape([-1, 1])\n",
    "    if len(mat_Y.shape) == 1:\n",
    "        mat_Y = mat_Y.reshape([-1, 1])\n",
    "\n",
    "    # Convert to sparse matrix if not already sparse\n",
    "    if sp.sparse.issparse(mat_X) is False:\n",
    "        mat_X = sp.sparse.csr_matrix(mat_X)\n",
    "    if sp.sparse.issparse(mat_Y) is False:\n",
    "        mat_Y = sp.sparse.csr_matrix(mat_Y)\n",
    "\n",
    "    # Compute v_mean,v_var\n",
    "    v_X_mean, v_X_var = scdrs.pp._get_mean_var(mat_X, axis=0)\n",
    "    v_X_sd = np.sqrt(v_X_var).clip(min=1e-8)\n",
    "    v_Y_mean, v_Y_var = scdrs.pp._get_mean_var(mat_Y, axis=0)\n",
    "    v_Y_sd = np.sqrt(v_Y_var).clip(min=1e-8)\n",
    "\n",
    "    mat_corr = mat_X.T.dot(mat_Y) / mat_X.shape[0]\n",
    "    mat_corr = mat_corr - v_X_mean.reshape([-1, 1]).dot(v_Y_mean.reshape([1, -1]))\n",
    "    mat_corr = mat_corr / v_X_sd.reshape([-1, 1]).dot(v_Y_sd.reshape([1, -1]))\n",
    "    mat_corr = np.array(mat_corr, dtype=np.float32)\n",
    "\n",
    "    if (mat_X.shape[1] == 1) | (mat_Y.shape[1] == 1):\n",
    "        return mat_corr.reshape([-1])\n",
    "    else:\n",
    "        return mat_corr\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "############################## Archived functions ############################\n",
    "##############################################################################\n",
    "def correlate_gene(\n",
    "    data, trs_name=\"trs_ez\", suffix=\"\", corr_opt=\"pearson\", cov_list=None, copy=False\n",
    "):\n",
    "\n",
    "    \"\"\"Compute the correlation between gene expressions and TRS\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    data (n_cell, n_gene) : AnnData\n",
    "        adata.X should contain size-normalized log1p transformed count data\n",
    "    trs_name : str\n",
    "        The variable to correlate gene expression with. Should be one column in data.obs.\n",
    "    suffix : str\n",
    "        The name of the added gene-wise correlation would be 'trs_corr'+suffix.\n",
    "    corr_opt : str\n",
    "        Option for computing the correlation\n",
    "        'pearson': Pearson's correlation\n",
    "        'spearman': Spearman's correlation\n",
    "    cov_list : list of str\n",
    "        Covariates to control for.\n",
    "        The covariates are first centered and then regressed out from\n",
    "            both trs_name and the gene expression before computing the correlation.\n",
    "        Elements in cov_list should be present in data.obs.columns\n",
    "    copy : bool\n",
    "        If to make copy of the AnnData object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata (AnnData):\n",
    "        Add the columns 'scdrs_corr'+suffix to data.var\n",
    "    \"\"\"\n",
    "\n",
    "    adata = data.copy() if copy else data\n",
    "\n",
    "    # Check options\n",
    "    corr_opt_list = [\"pearson\", \"spearman\"]\n",
    "    if corr_opt not in corr_opt_list:\n",
    "        raise ValueError(\n",
    "            \"# compute_scdrs_corr: corr_opt not in [%s]\"\n",
    "            % \", \".join([str(x) for x in corr_opt_list])\n",
    "        )\n",
    "    if trs_name not in adata.obs.columns:\n",
    "        raise ValueError(\"# compute_scdrs_corr: %s not in data.obs.columns\" % trs_name)\n",
    "    if cov_list is not None:\n",
    "        temp_list = list(set(cov_list) - set(adata.obs.columns))\n",
    "        if len(temp_list) > 0:\n",
    "            raise ValueError(\n",
    "                \"# compute_scdrs_corr: covariates %s not in data.obs.columns\"\n",
    "                % \",\".join(temp_list)\n",
    "            )\n",
    "\n",
    "    # Get data\n",
    "    mat_X = data.X.toarray()\n",
    "    v_trs = data.obs[trs_name].values.copy()\n",
    "\n",
    "    # Regress out covariates\n",
    "    if cov_list is not None:\n",
    "        mat_cov = adata.obs[cov_list].values.copy()\n",
    "        mat_cov = mat_cov - mat_cov.mean(axis=0)\n",
    "        v_trs = scdrs.pp.reg_out(v_trs, mat_cov)\n",
    "        mat_X = scdrs.pp.reg_out(mat_X, mat_cov)\n",
    "\n",
    "    # Compute correlation\n",
    "    if corr_opt == \"pearson\":\n",
    "        v_corr = _pearson_corr(mat_X, v_trs)\n",
    "\n",
    "    if corr_opt == \"spearman\":\n",
    "        v_corr = _spearman_corr(mat_X, v_trs)\n",
    "\n",
    "    adata.var[\"scdrs_corr\" + suffix] = v_corr\n",
    "\n",
    "    return adata if copy else None\n",
    "\n",
    "\n",
    "def _spearman_corr(mat_X, mat_Y):\n",
    "    \"\"\"Spearman's correlation between every columns in mat_X and mat_Y\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    mat_X (N,M1): np.ndarray\n",
    "    mat_Y (N,M2): np.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mat_corr (M1,M2): np.ndarray\n",
    "        Correlation matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape\n",
    "    if len(mat_X.shape) == 1:\n",
    "        mat_X = mat_X.reshape([-1, 1])\n",
    "    if len(mat_Y.shape) == 1:\n",
    "        mat_Y = mat_Y.reshape([-1, 1])\n",
    "\n",
    "    mat_X = _get_rank(mat_X, axis=0)\n",
    "    mat_Y = _get_rank(mat_Y, axis=0)\n",
    "\n",
    "    mat_X = (mat_X - mat_X.mean(axis=0)) / mat_X.std(axis=0).clip(min=1e-8)\n",
    "    mat_Y = (mat_Y - mat_Y.mean(axis=0)) / mat_Y.std(axis=0).clip(min=1e-8)\n",
    "    mat_corr = mat_X.T.dot(mat_Y) / mat_X.shape[0]\n",
    "\n",
    "    if (mat_X.shape[1] == 1) | (mat_Y.shape[1] == 1):\n",
    "        return mat_corr.reshape([-1])\n",
    "    else:\n",
    "        return mat_corr\n",
    "\n",
    "\n",
    "def _get_rank(mat_X, axis=0):\n",
    "    \"\"\"Get rank for each row/columns of the given matrix\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    mat_X (N,M): np.ndarray\n",
    "    axis: int\n",
    "        axis=0: column-wise rank (across rows)\n",
    "        axis=1: row-wise rank (across columns)\n",
    "    Returns\n",
    "    -------\n",
    "    mat_rank  (N,M): np.ndarray\n",
    "        Rank matrix\n",
    "    \"\"\"\n",
    "\n",
    "    if axis == 0:\n",
    "        mat_X = np.argsort(mat_X, axis=0)\n",
    "        mat_rank = np.empty_like(mat_X)\n",
    "        temp_v = np.arange(mat_X.shape[0])\n",
    "        for i_col in range(mat_X.shape[1]):\n",
    "            mat_rank[mat_X[:, i_col], i_col] = temp_v\n",
    "\n",
    "    if axis == 1:\n",
    "        mat_X = np.argsort(mat_X, axis=1)\n",
    "        mat_rank = np.empty_like(mat_X)\n",
    "        temp_v = np.arange(mat_X.shape[1])\n",
    "        for i_row in range(mat_X.shape[0]):\n",
    "            mat_rank[i_row, mat_X[i_row, :]] = temp_v\n",
    "\n",
    "    return mat_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AnnData object with sampled 100,000 cells\n",
    "adata = sc.read_h5ad('/scRNA/benchmarking_100000cells.h5ad')\n",
    "adata.X = adata.layers['data']\n",
    "del adata.layers\n",
    "scdrs.preprocess(adata, cov=sc_cov)\n",
    "\n",
    "# Scoring and get p-value distributions\n",
    "for gene_num in gene_nums:\n",
    "    scDRS_pvals = pd.DataFrame(index=adata.obs.index)\n",
    "    for n in range(1,101):\n",
    "        gene_rep_null = pd.read_csv(f\"/Benchmark/Null_simulation/Null_SBP_new/null{n}_PLS_report.csv\",index_col=0)\n",
    "        gene_rep_null = gene_rep_null[gene_rep_null[\"Sign\"]>0]\n",
    "        gene_list = list(gene_rep_null.head(gene_num).index)\n",
    "        gene_weight = gene_rep_null['Weight'].values\n",
    "        scDRS_null_n = score_cell_parallel(adata, gene_list, gene_weight, n_genebin=50)\n",
    "        scDRS_pvals[f\"pval_{n}\"] = scDRS_null_n[\"pval\"]\n",
    "    scDRS_pvals.to_csv(f\"/Benchmark/Null_simulation/scDRS_pvals_{gene_num}genes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f08bc",
   "metadata": {},
   "source": [
    "ZOOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AnnData object with sampled 100,000 cells\n",
    "adata = sc.read_h5ad('/scRNA/benchmarking_100000cells.h5ad')\n",
    "adata.X = adata.layers['gss']\n",
    "del adata.layers\n",
    "\n",
    "# Scoring and get p-value distributions\n",
    "for gene_num in gene_nums:\n",
    "    ZOOM_pvals = pd.DataFrame(index=adata.obs.index)\n",
    "    for n in range(1,101):\n",
    "        gene_rep_null = pd.read_csv(f\"/Benchmark/Null_simulation/Null_SBP_new/null{n}_PLS_report.csv\",index_col=0)\n",
    "        weight_perm_null = pd.read_csv(f\"/Benchmark/Null_simulation/Null_SBP/null{n}_weight_perm.csv\",index_col=0)\n",
    "        sign_perm_null = pd.read_csv(f\"/Benchmark/Null_simulation/Null_SBP/null{n}_sign_perm.csv\",index_col=0)\n",
    "        ZOOM_null_n = sct.score_cell_zoom(\n",
    "            adata, gene_rep_null, \"Weight\", \"Sign\", \"p_perm\",\n",
    "            weight_perm_null, sign_perm_null, True, gene_num,\n",
    "            \"gss_max\", \"DS\", 20, False, False, -1\n",
    "        )\n",
    "        ZOOM_pvals[f\"pval_{n}\"] = ZOOM_null_n[\"pval\"]\n",
    "    ZOOM_pvals.to_csv(f\"/Benchmark/Null_simulation/ZOOM_pvals_{gene_num}genes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b108fc",
   "metadata": {},
   "source": [
    "As for UCell and VAM, the scoring functions are only built on R, see Null_simulation.R for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15896673",
   "metadata": {},
   "source": [
    "Now, we can calculate the p-value distributions generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c920ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help function for the calculation of p-value quantiles\n",
    "def p_quantile(pvals):\n",
    "    ns = [round(i*0.1, 1) for i in range(1, 31)]\n",
    "    quantiles = [10**(-n) for n in ns]\n",
    "    act_qual = []\n",
    "    for q in quantiles:\n",
    "        quantile_value = pvals.quantile(q)\n",
    "        act_qual.append(-np.log10(quantile_value))\n",
    "    return act_qual\n",
    "\n",
    "# Help function for the calculation of confidence intervals\n",
    "def row_mean_ci(df, ci=0.95):\n",
    "    results = []\n",
    "    alpha = 1 - ci\n",
    "    for idx, row in df.iterrows():\n",
    "        vals = row.dropna().values\n",
    "        n = vals.size\n",
    "        if n == 0:\n",
    "            results.append((np.nan, np.nan, np.nan))\n",
    "            continue\n",
    "        mean = vals.mean()\n",
    "        if n == 1:\n",
    "            results.append((mean, np.nan, np.nan))\n",
    "            continue\n",
    "        sem = stats.sem(vals, nan_policy='omit')\n",
    "        t_crit = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "        margin = t_crit * sem\n",
    "        results.append((mean, mean - margin, mean + margin))\n",
    "    out = pd.DataFrame(results, index=df.index, columns=['mean', 'ci_lower', 'ci_upper'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0/100000\n",
    "for gene_num in gene_nums:\n",
    "    Null_Seurat = pd.read_csv(f'/Benchmark/Null_simulation/Seurat_pvals_{gene_num}genes.csv',index_col=0)\n",
    "    Null_Seurat = Null_Seurat.clip(lower=threshold)\n",
    "    Null_VISION = pd.read_csv(f'/Benchmark/Null_simulation/VISION_pvals_{gene_num}genes.csv',index_col=0)\n",
    "    Null_VISION = Null_VISION.clip(lower=threshold)\n",
    "    Null_scDRS = pd.read_csv(f'/Benchmark/Null_simulation/scDRS_pvals_{gene_num}genes.csv',index_col=0)\n",
    "    Null_scDRS = Null_scDRS.clip(lower=threshold)\n",
    "    Null_ZOOM = pd.read_csv(f'/Benchmark/Null_simulation/scDRS_modified_pvals_{gene_num}genes.csv',index_col=0)\n",
    "    Null_ZOOM = Null_ZOOM.clip(lower=threshold)\n",
    "    Null_VAM = pd.read_csv(f'/Benchmark/Null_simulation/VAM_pvals_{gene_num}genes.csv',index_col=0)\n",
    "    Null_VAM = Null_VAM.clip(lower=threshold)\n",
    "    Null_UCell = pd.read_csv(f'/Benchmark/Null_simulation/UCell_pvals_{gene_num}genes.csv',index_col=0)\n",
    "    Null_UCell = Null_UCell.clip(lower=threshold)\n",
    "    Seurat_qual = pd.DataFrame(index=[round(i*0.1, 1) for i in range(1, 31)])\n",
    "    VISION_qual = pd.DataFrame(index=[round(i*0.1, 1) for i in range(1, 31)])\n",
    "    scDRS_qual = pd.DataFrame(index=[round(i*0.1, 1) for i in range(1, 31)])\n",
    "    ZOOM_qual = pd.DataFrame(index=[round(i*0.1, 1) for i in range(1, 31)])\n",
    "    VAM_qual = pd.DataFrame(index=[round(i*0.1, 1) for i in range(1, 31)])\n",
    "    UCell_qual = pd.DataFrame(index=[round(i*0.1, 1) for i in range(1, 31)])\n",
    "    for n in range(1,101):\n",
    "        pvals = Null_Seurat[f\"pval_{n}\"]\n",
    "        p_qual = p_quantile(pvals)\n",
    "        Seurat_qual[f\"qual_{n}\"] = p_qual\n",
    "        pvals = Null_VISION[f\"pval_{n}\"]\n",
    "        p_qual = p_quantile(pvals)\n",
    "        VISION_qual[f\"qual_{n}\"] = p_qual\n",
    "        pvals = Null_scDRS[f\"pval_{n}\"]\n",
    "        p_qual = p_quantile(pvals)\n",
    "        scDRS_qual[f\"qual_{n}\"] = p_qual\n",
    "        pvals = Null_ZOOM[f\"pval_{n}\"]\n",
    "        p_qual = p_quantile(pvals)\n",
    "        ZOOM_qual[f\"qual_{n}\"] = p_qual\n",
    "        pvals = Null_VAM[f\"Null_{n}\"]\n",
    "        p_qual = p_quantile(pvals)\n",
    "        VAM_qual[f\"qual_{n}\"] = p_qual\n",
    "        pvals = Null_UCell[f\"pval_{n}\"]\n",
    "        p_qual = p_quantile(pvals)\n",
    "        UCell_qual[f\"qual_{n}\"] = p_qual\n",
    "        \n",
    "    null_simulation_dict = {\n",
    "        \"Seurat\": row_mean_ci(Seurat_qual),\n",
    "        \"VISION\": row_mean_ci(VISION_qual),\n",
    "        \"VAM\": row_mean_ci(VAM_qual),\n",
    "        \"UCell\": row_mean_ci(UCell_qual),\n",
    "        \"scDRS\": row_mean_ci(scDRS_qual),\n",
    "        \"ZOOM\": row_mean_ci(ZOOM_qual)}\n",
    "    null_simulation_dfs = []\n",
    "    for method, df in null_simulation_dict.items():\n",
    "        df[\"quantile\"] = df.index\n",
    "        df[\"method\"] = method\n",
    "        null_simulation_dfs.append(df)\n",
    "    null_simulation_df = pd.concat(null_simulation_dfs, axis=0).reset_index(drop=False)\n",
    "    null_simulation_df.index = range(1, len(null_simulation_df) + 1)\n",
    "    null_simulation_df.to_csv(f\"/Benchmark/Null_simulation/Null_simulation_{gene_num}genes.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC3.9",
   "language": "python",
   "name": "dlc3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
